This is a simple tutorial for adding a task to the framework. 
## First
reate your benchmark.py in ./tasks/utils/benchmark_class and inherit from BASE in base_class.The following is an example of the requirements for some of the code.
### Class
```python
llm_param = {
}
metric1 = {"metric_name1":None} #The None position can be a dictionary that stores the parameters required for evaluation
Class benchmark(Base):
    def __init__(self,limit):
        super().__init__()
        benchmark_name = "benchmark_name" 
        task_names = ["task_name1"]
        ability = "Reasoning"
        data_path = f"tasks/{ability}/{benchmark_name}/data"
        limit = int(limit) if limit!="auto" else 10000
        hf = ""  #for download data
        llm_params = {"task_name1":llm_paramm} #The llm_params attribute is a dictionary where the keys are the task names specified in task_names, and the values are the parameters for model inference corresponding to each task
        metric = {"task_name1":metric1} #The parameter is a string. Please add it to the METRICS_REGISTRY function in lab/lte/metrics/__init__.py for importing the evaluation metrics
``` 
### evaluation metrics
The evaluation functions obtained through METRICS_REGISTRY are as follows:

``` python
class metric_name1:
    def __init__(self,**kwargs):
        pass
    def __call__(self, choices, ground_truths, results):
        return score
``` 
- choices: This refers to the options provided in a multiple-choice question.
- ground_truths: These are the correct answers to the questions, whether they are multiple-choice or open-ended.
- results: These are the outputs generated by the model in response to the questions.
### Function
1. Data Download and Conversion (Simplified Code)
After downloading the data to the path "./tasks/{}/{}/tmp_Rawdata".format(ability, benchmark_name), convert it into the data format required by the framework and save it to "./tasks/{}/{}/data/{}.json".format(ability, self.benchmark_name, task_name).

``` python
def download_and_transform_data(self,**kwargs):
     load_dataset()
    self.make_data()
    # After downloading the raw data to the specified path, convert it into the required format.
def make_data(self,**kwargs):
    for data in "input_path": #Iterate through the downloaded data and convert it into the required format.
        transform_data(data)
        save in output_path
def transform_data(self,raw_data):
    return {
        "passage": raw_data["passage"],
        "question": raw_data["question"],
        "choices": "",
        "answer": raw_data["reference_counting_results"],
    }
``` 

2. Data Processing
``` python
# Process the data input to the model, modify the format, and add prompts.
def transform(self,data,task_name,**kwargs):
    prompt_list = {
    "task_name": prompt:。\n\n{context}\n\n****\n\n{input}\n\n******\n\n******：",
}
    return {
        "input": prompt_list[task_name].format(),
        "output": data["answer"],
        "processed_output": data["answer"],
    }
##For different benchmarks, the data processing before inputting into the model varies. Here is an example of setting up a modify function to handle the data:
def modify(self, prompt, model, model_path,**kwargs):
    """Adjust input prompt to fit within the model's token limit."""
    if hasattr(model.tokenizer, 'apply_chat_template'):
        prompt = model.tokenizer.apply_chat_template(
            [{"role": "user", "content": prompt}],
            tokenize=False, add_generation_prompt=True
        )
    return prompt
```

## Second
After completing the code, configure the benchmark configuration file at ./tasks/{ability}/{benchmark_name}/benchmark_name.yaml.
## Last 
run code 

``` bash
lte.run --model_path your model_path \
    --eval \
    --benchmark_configs your config path:./tasks/{ability}/{benchmark_name}/
``` 